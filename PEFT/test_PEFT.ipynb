{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "# import deeplabv3_resnet50\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Use the values from the configuration file\n",
    "dataset_path = config['data_path']\n",
    "num_epochs = config['num_epochs']\n",
    "save_dir = config['save_dir']\n",
    "continue_training = config['continue_training']\n",
    "\n",
    "num_classes = 20\n",
    "model = deeplabv3_resnet50(weights=None, num_classes=20, aux_loss=True)\n",
    "# Number of effective classes after mapping (19 classes + 1 background)\n",
    "\n",
    "# Replace the classifier of the model\n",
    "\n",
    "# Mapping for reducing classes to 20 including background\n",
    "mapping_20 = {\n",
    "    0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 2, 9: 0,\n",
    "    10: 0, 11: 3, 12: 4, 13: 5, 14: 0, 15: 0, 16: 0, 17: 6, 18: 0,\n",
    "    19: 7, 20: 8, 21: 9, 22: 10, 23: 11, 24: 12, 25: 13, 26: 14,\n",
    "    27: 15, 28: 16, 29: 0, 30: 0, 31: 17, 32: 18, 33: 19, -1: 0\n",
    "}\n",
    "\n",
    "def encode_labels(mask):\n",
    "    label_mask = np.zeros_like(mask)\n",
    "    for k in mapping_20:\n",
    "        label_mask[mask == k] = mapping_20[k]\n",
    "    return label_mask\n",
    "\n",
    "def transform_target(target):\n",
    "    target = np.array(target)  # Convert PIL Image to numpy array\n",
    "    target = encode_labels(target)  # Remap labels\n",
    "    return torch.as_tensor(target, dtype=torch.int64)  # Convert numpy array to tensor\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transform_target\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the dataset with appropriate transforms for both images and targets\n",
    "train_dataset = datasets.Cityscapes(root=dataset_path, split='train', mode='fine', target_type='semantic',\n",
    "                                    transform=transform, target_transform=target_transform)\n",
    "val_dataset = datasets.Cityscapes(root=dataset_path, split='val', mode='fine', target_type='semantic',\n",
    "                                  transform=transform, target_transform=target_transform)\n",
    "# test_dataset = datasets.Cityscapes(root=dataset_path, split='test', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "# batch size should be set to 4 or more on GPU for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, drop_last=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"training on\", device)\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "learning_rate = 0.001  # Initial learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # learning rate decayï¼Œreduce the learning rate by a factor of 0.1 every 7 epochs\n",
    "\n",
    "if continue_training:\n",
    "    model.load_state_dict(torch.load(save_dir +'/best_model_weights.pth'))\n",
    "\n",
    "# Training loop\n",
    "train_loss_list = [] # total loss\n",
    "val_loss_list = [] # total # loss\n",
    "best_val_loss = float('inf')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T07:58:04.768027Z",
     "start_time": "2024-04-26T07:58:03.862120Z"
    }
   },
   "id": "1a7a3ea71eebc0bd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 42003560 || all params: 42003560 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T07:58:58.003947Z",
     "start_time": "2024-04-26T07:58:57.979401Z"
    }
   },
   "id": "feb87a784a781c93"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'query', 'value'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 11\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraConfig, get_peft_model\n\u001B[1;32m      3\u001B[0m config \u001B[38;5;241m=\u001B[39m LoraConfig(\n\u001B[1;32m      4\u001B[0m     r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m,\n\u001B[1;32m      5\u001B[0m     lora_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     modules_to_save\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassifier\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 11\u001B[0m lora_model \u001B[38;5;241m=\u001B[39m \u001B[43mget_peft_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m print_trainable_parameters(lora_model)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/mapping.py:141\u001B[0m, in \u001B[0;36mget_peft_model\u001B[0;34m(model, peft_config, adapter_name, mixed)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PeftMixedModel(model, peft_config, adapter_name\u001B[38;5;241m=\u001B[39madapter_name)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m peft_config\u001B[38;5;241m.\u001B[39mtask_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m peft_config\u001B[38;5;241m.\u001B[39mis_prompt_learning:\n\u001B[0;32m--> 141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPeftModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m peft_config\u001B[38;5;241m.\u001B[39mis_prompt_learning:\n\u001B[1;32m    144\u001B[0m     peft_config \u001B[38;5;241m=\u001B[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/peft_model.py:135\u001B[0m, in \u001B[0;36mPeftModel.__init__\u001B[0;34m(self, model, peft_config, adapter_name)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_peft_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001B[38;5;241m.\u001B[39mpeft_type]\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_gradient_checkpointing\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/tuners/lora/model.py:136\u001B[0m, in \u001B[0;36mLoraModel.__init__\u001B[0;34m(self, model, config, adapter_name)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:166\u001B[0m, in \u001B[0;36mBaseTuner.__init__\u001B[0;34m(self, model, peft_config, adapter_name)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m adapter_name\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_injection_hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config[adapter_name], adapter_name)\n\u001B[0;32m--> 166\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minject_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpeft_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:361\u001B[0m, in \u001B[0;36mBaseTuner.inject_adapter\u001B[0;34m(self, model, adapter_name)\u001B[0m\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001B[38;5;241m=\u001B[39mkey)\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_target_modules_in_base_model:\n\u001B[0;32m--> 361\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget modules \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeft_config\u001B[38;5;241m.\u001B[39mtarget_modules\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in the base model. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check the target modules and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    364\u001B[0m     )\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mark_only_adapters_as_trainable(model)\n\u001B[1;32m    368\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config[adapter_name]\u001B[38;5;241m.\u001B[39minference_mode:\n",
      "\u001B[0;31mValueError\u001B[0m: Target modules {'query', 'value'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T08:02:03.608232Z",
     "start_time": "2024-04-26T08:02:03.411053Z"
    }
   },
   "id": "eb16bc8c61022e1b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85800194 || all params: 85800194 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"  # pre-trained model from which to fine-tune\n",
    "\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T08:08:09.520824Z",
     "start_time": "2024-04-26T08:08:04.090494Z"
    }
   },
   "id": "7b3c766b8b847b26"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591362 || all params: 86391556 || trainable%: 0.68\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T08:08:46.848455Z",
     "start_time": "2024-04-26T08:08:46.741539Z"
    }
   },
   "id": "1b341215ae20d4ee"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vit\n",
      "vit.embeddings\n",
      "vit.embeddings.patch_embeddings\n",
      "vit.embeddings.patch_embeddings.projection\n",
      "vit.embeddings.dropout\n",
      "vit.encoder\n",
      "vit.encoder.layer\n",
      "vit.encoder.layer.0\n",
      "vit.encoder.layer.0.attention\n",
      "vit.encoder.layer.0.attention.attention\n",
      "vit.encoder.layer.0.attention.attention.query\n",
      "vit.encoder.layer.0.attention.attention.query.base_layer\n",
      "vit.encoder.layer.0.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.0.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.0.attention.attention.query.lora_A\n",
      "vit.encoder.layer.0.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.0.attention.attention.query.lora_B\n",
      "vit.encoder.layer.0.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.0.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.0.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.0.attention.attention.key\n",
      "vit.encoder.layer.0.attention.attention.value\n",
      "vit.encoder.layer.0.attention.attention.value.base_layer\n",
      "vit.encoder.layer.0.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.0.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.0.attention.attention.value.lora_A\n",
      "vit.encoder.layer.0.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.0.attention.attention.value.lora_B\n",
      "vit.encoder.layer.0.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.0.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.0.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.0.attention.attention.dropout\n",
      "vit.encoder.layer.0.attention.output\n",
      "vit.encoder.layer.0.attention.output.dense\n",
      "vit.encoder.layer.0.attention.output.dropout\n",
      "vit.encoder.layer.0.intermediate\n",
      "vit.encoder.layer.0.intermediate.dense\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.0.output\n",
      "vit.encoder.layer.0.output.dense\n",
      "vit.encoder.layer.0.output.dropout\n",
      "vit.encoder.layer.0.layernorm_before\n",
      "vit.encoder.layer.0.layernorm_after\n",
      "vit.encoder.layer.1\n",
      "vit.encoder.layer.1.attention\n",
      "vit.encoder.layer.1.attention.attention\n",
      "vit.encoder.layer.1.attention.attention.query\n",
      "vit.encoder.layer.1.attention.attention.query.base_layer\n",
      "vit.encoder.layer.1.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.1.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.1.attention.attention.query.lora_A\n",
      "vit.encoder.layer.1.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.1.attention.attention.query.lora_B\n",
      "vit.encoder.layer.1.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.1.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.1.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.1.attention.attention.key\n",
      "vit.encoder.layer.1.attention.attention.value\n",
      "vit.encoder.layer.1.attention.attention.value.base_layer\n",
      "vit.encoder.layer.1.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.1.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.1.attention.attention.value.lora_A\n",
      "vit.encoder.layer.1.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.1.attention.attention.value.lora_B\n",
      "vit.encoder.layer.1.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.1.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.1.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.1.attention.attention.dropout\n",
      "vit.encoder.layer.1.attention.output\n",
      "vit.encoder.layer.1.attention.output.dense\n",
      "vit.encoder.layer.1.attention.output.dropout\n",
      "vit.encoder.layer.1.intermediate\n",
      "vit.encoder.layer.1.intermediate.dense\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.1.output\n",
      "vit.encoder.layer.1.output.dense\n",
      "vit.encoder.layer.1.output.dropout\n",
      "vit.encoder.layer.1.layernorm_before\n",
      "vit.encoder.layer.1.layernorm_after\n",
      "vit.encoder.layer.2\n",
      "vit.encoder.layer.2.attention\n",
      "vit.encoder.layer.2.attention.attention\n",
      "vit.encoder.layer.2.attention.attention.query\n",
      "vit.encoder.layer.2.attention.attention.query.base_layer\n",
      "vit.encoder.layer.2.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.2.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.2.attention.attention.query.lora_A\n",
      "vit.encoder.layer.2.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.2.attention.attention.query.lora_B\n",
      "vit.encoder.layer.2.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.2.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.2.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.2.attention.attention.key\n",
      "vit.encoder.layer.2.attention.attention.value\n",
      "vit.encoder.layer.2.attention.attention.value.base_layer\n",
      "vit.encoder.layer.2.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.2.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.2.attention.attention.value.lora_A\n",
      "vit.encoder.layer.2.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.2.attention.attention.value.lora_B\n",
      "vit.encoder.layer.2.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.2.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.2.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.2.attention.attention.dropout\n",
      "vit.encoder.layer.2.attention.output\n",
      "vit.encoder.layer.2.attention.output.dense\n",
      "vit.encoder.layer.2.attention.output.dropout\n",
      "vit.encoder.layer.2.intermediate\n",
      "vit.encoder.layer.2.intermediate.dense\n",
      "vit.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.2.output\n",
      "vit.encoder.layer.2.output.dense\n",
      "vit.encoder.layer.2.output.dropout\n",
      "vit.encoder.layer.2.layernorm_before\n",
      "vit.encoder.layer.2.layernorm_after\n",
      "vit.encoder.layer.3\n",
      "vit.encoder.layer.3.attention\n",
      "vit.encoder.layer.3.attention.attention\n",
      "vit.encoder.layer.3.attention.attention.query\n",
      "vit.encoder.layer.3.attention.attention.query.base_layer\n",
      "vit.encoder.layer.3.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.3.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.3.attention.attention.query.lora_A\n",
      "vit.encoder.layer.3.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.3.attention.attention.query.lora_B\n",
      "vit.encoder.layer.3.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.3.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.3.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.3.attention.attention.key\n",
      "vit.encoder.layer.3.attention.attention.value\n",
      "vit.encoder.layer.3.attention.attention.value.base_layer\n",
      "vit.encoder.layer.3.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.3.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.3.attention.attention.value.lora_A\n",
      "vit.encoder.layer.3.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.3.attention.attention.value.lora_B\n",
      "vit.encoder.layer.3.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.3.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.3.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.3.attention.attention.dropout\n",
      "vit.encoder.layer.3.attention.output\n",
      "vit.encoder.layer.3.attention.output.dense\n",
      "vit.encoder.layer.3.attention.output.dropout\n",
      "vit.encoder.layer.3.intermediate\n",
      "vit.encoder.layer.3.intermediate.dense\n",
      "vit.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.3.output\n",
      "vit.encoder.layer.3.output.dense\n",
      "vit.encoder.layer.3.output.dropout\n",
      "vit.encoder.layer.3.layernorm_before\n",
      "vit.encoder.layer.3.layernorm_after\n",
      "vit.encoder.layer.4\n",
      "vit.encoder.layer.4.attention\n",
      "vit.encoder.layer.4.attention.attention\n",
      "vit.encoder.layer.4.attention.attention.query\n",
      "vit.encoder.layer.4.attention.attention.query.base_layer\n",
      "vit.encoder.layer.4.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.4.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.4.attention.attention.query.lora_A\n",
      "vit.encoder.layer.4.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.4.attention.attention.query.lora_B\n",
      "vit.encoder.layer.4.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.4.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.4.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.4.attention.attention.key\n",
      "vit.encoder.layer.4.attention.attention.value\n",
      "vit.encoder.layer.4.attention.attention.value.base_layer\n",
      "vit.encoder.layer.4.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.4.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.4.attention.attention.value.lora_A\n",
      "vit.encoder.layer.4.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.4.attention.attention.value.lora_B\n",
      "vit.encoder.layer.4.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.4.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.4.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.4.attention.attention.dropout\n",
      "vit.encoder.layer.4.attention.output\n",
      "vit.encoder.layer.4.attention.output.dense\n",
      "vit.encoder.layer.4.attention.output.dropout\n",
      "vit.encoder.layer.4.intermediate\n",
      "vit.encoder.layer.4.intermediate.dense\n",
      "vit.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.4.output\n",
      "vit.encoder.layer.4.output.dense\n",
      "vit.encoder.layer.4.output.dropout\n",
      "vit.encoder.layer.4.layernorm_before\n",
      "vit.encoder.layer.4.layernorm_after\n",
      "vit.encoder.layer.5\n",
      "vit.encoder.layer.5.attention\n",
      "vit.encoder.layer.5.attention.attention\n",
      "vit.encoder.layer.5.attention.attention.query\n",
      "vit.encoder.layer.5.attention.attention.query.base_layer\n",
      "vit.encoder.layer.5.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.5.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.5.attention.attention.query.lora_A\n",
      "vit.encoder.layer.5.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.5.attention.attention.query.lora_B\n",
      "vit.encoder.layer.5.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.5.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.5.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.5.attention.attention.key\n",
      "vit.encoder.layer.5.attention.attention.value\n",
      "vit.encoder.layer.5.attention.attention.value.base_layer\n",
      "vit.encoder.layer.5.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.5.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.5.attention.attention.value.lora_A\n",
      "vit.encoder.layer.5.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.5.attention.attention.value.lora_B\n",
      "vit.encoder.layer.5.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.5.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.5.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.5.attention.attention.dropout\n",
      "vit.encoder.layer.5.attention.output\n",
      "vit.encoder.layer.5.attention.output.dense\n",
      "vit.encoder.layer.5.attention.output.dropout\n",
      "vit.encoder.layer.5.intermediate\n",
      "vit.encoder.layer.5.intermediate.dense\n",
      "vit.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.5.output\n",
      "vit.encoder.layer.5.output.dense\n",
      "vit.encoder.layer.5.output.dropout\n",
      "vit.encoder.layer.5.layernorm_before\n",
      "vit.encoder.layer.5.layernorm_after\n",
      "vit.encoder.layer.6\n",
      "vit.encoder.layer.6.attention\n",
      "vit.encoder.layer.6.attention.attention\n",
      "vit.encoder.layer.6.attention.attention.query\n",
      "vit.encoder.layer.6.attention.attention.query.base_layer\n",
      "vit.encoder.layer.6.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.6.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.6.attention.attention.query.lora_A\n",
      "vit.encoder.layer.6.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.6.attention.attention.query.lora_B\n",
      "vit.encoder.layer.6.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.6.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.6.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.6.attention.attention.key\n",
      "vit.encoder.layer.6.attention.attention.value\n",
      "vit.encoder.layer.6.attention.attention.value.base_layer\n",
      "vit.encoder.layer.6.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.6.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.6.attention.attention.value.lora_A\n",
      "vit.encoder.layer.6.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.6.attention.attention.value.lora_B\n",
      "vit.encoder.layer.6.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.6.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.6.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.6.attention.attention.dropout\n",
      "vit.encoder.layer.6.attention.output\n",
      "vit.encoder.layer.6.attention.output.dense\n",
      "vit.encoder.layer.6.attention.output.dropout\n",
      "vit.encoder.layer.6.intermediate\n",
      "vit.encoder.layer.6.intermediate.dense\n",
      "vit.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.6.output\n",
      "vit.encoder.layer.6.output.dense\n",
      "vit.encoder.layer.6.output.dropout\n",
      "vit.encoder.layer.6.layernorm_before\n",
      "vit.encoder.layer.6.layernorm_after\n",
      "vit.encoder.layer.7\n",
      "vit.encoder.layer.7.attention\n",
      "vit.encoder.layer.7.attention.attention\n",
      "vit.encoder.layer.7.attention.attention.query\n",
      "vit.encoder.layer.7.attention.attention.query.base_layer\n",
      "vit.encoder.layer.7.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.7.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.7.attention.attention.query.lora_A\n",
      "vit.encoder.layer.7.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.7.attention.attention.query.lora_B\n",
      "vit.encoder.layer.7.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.7.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.7.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.7.attention.attention.key\n",
      "vit.encoder.layer.7.attention.attention.value\n",
      "vit.encoder.layer.7.attention.attention.value.base_layer\n",
      "vit.encoder.layer.7.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.7.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.7.attention.attention.value.lora_A\n",
      "vit.encoder.layer.7.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.7.attention.attention.value.lora_B\n",
      "vit.encoder.layer.7.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.7.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.7.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.7.attention.attention.dropout\n",
      "vit.encoder.layer.7.attention.output\n",
      "vit.encoder.layer.7.attention.output.dense\n",
      "vit.encoder.layer.7.attention.output.dropout\n",
      "vit.encoder.layer.7.intermediate\n",
      "vit.encoder.layer.7.intermediate.dense\n",
      "vit.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.7.output\n",
      "vit.encoder.layer.7.output.dense\n",
      "vit.encoder.layer.7.output.dropout\n",
      "vit.encoder.layer.7.layernorm_before\n",
      "vit.encoder.layer.7.layernorm_after\n",
      "vit.encoder.layer.8\n",
      "vit.encoder.layer.8.attention\n",
      "vit.encoder.layer.8.attention.attention\n",
      "vit.encoder.layer.8.attention.attention.query\n",
      "vit.encoder.layer.8.attention.attention.query.base_layer\n",
      "vit.encoder.layer.8.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.8.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.8.attention.attention.query.lora_A\n",
      "vit.encoder.layer.8.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.8.attention.attention.query.lora_B\n",
      "vit.encoder.layer.8.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.8.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.8.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.8.attention.attention.key\n",
      "vit.encoder.layer.8.attention.attention.value\n",
      "vit.encoder.layer.8.attention.attention.value.base_layer\n",
      "vit.encoder.layer.8.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.8.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.8.attention.attention.value.lora_A\n",
      "vit.encoder.layer.8.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.8.attention.attention.value.lora_B\n",
      "vit.encoder.layer.8.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.8.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.8.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.8.attention.attention.dropout\n",
      "vit.encoder.layer.8.attention.output\n",
      "vit.encoder.layer.8.attention.output.dense\n",
      "vit.encoder.layer.8.attention.output.dropout\n",
      "vit.encoder.layer.8.intermediate\n",
      "vit.encoder.layer.8.intermediate.dense\n",
      "vit.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.8.output\n",
      "vit.encoder.layer.8.output.dense\n",
      "vit.encoder.layer.8.output.dropout\n",
      "vit.encoder.layer.8.layernorm_before\n",
      "vit.encoder.layer.8.layernorm_after\n",
      "vit.encoder.layer.9\n",
      "vit.encoder.layer.9.attention\n",
      "vit.encoder.layer.9.attention.attention\n",
      "vit.encoder.layer.9.attention.attention.query\n",
      "vit.encoder.layer.9.attention.attention.query.base_layer\n",
      "vit.encoder.layer.9.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.9.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.9.attention.attention.query.lora_A\n",
      "vit.encoder.layer.9.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.9.attention.attention.query.lora_B\n",
      "vit.encoder.layer.9.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.9.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.9.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.9.attention.attention.key\n",
      "vit.encoder.layer.9.attention.attention.value\n",
      "vit.encoder.layer.9.attention.attention.value.base_layer\n",
      "vit.encoder.layer.9.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.9.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.9.attention.attention.value.lora_A\n",
      "vit.encoder.layer.9.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.9.attention.attention.value.lora_B\n",
      "vit.encoder.layer.9.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.9.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.9.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.9.attention.attention.dropout\n",
      "vit.encoder.layer.9.attention.output\n",
      "vit.encoder.layer.9.attention.output.dense\n",
      "vit.encoder.layer.9.attention.output.dropout\n",
      "vit.encoder.layer.9.intermediate\n",
      "vit.encoder.layer.9.intermediate.dense\n",
      "vit.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.9.output\n",
      "vit.encoder.layer.9.output.dense\n",
      "vit.encoder.layer.9.output.dropout\n",
      "vit.encoder.layer.9.layernorm_before\n",
      "vit.encoder.layer.9.layernorm_after\n",
      "vit.encoder.layer.10\n",
      "vit.encoder.layer.10.attention\n",
      "vit.encoder.layer.10.attention.attention\n",
      "vit.encoder.layer.10.attention.attention.query\n",
      "vit.encoder.layer.10.attention.attention.query.base_layer\n",
      "vit.encoder.layer.10.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.10.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.10.attention.attention.query.lora_A\n",
      "vit.encoder.layer.10.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.10.attention.attention.query.lora_B\n",
      "vit.encoder.layer.10.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.10.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.10.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.10.attention.attention.key\n",
      "vit.encoder.layer.10.attention.attention.value\n",
      "vit.encoder.layer.10.attention.attention.value.base_layer\n",
      "vit.encoder.layer.10.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.10.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.10.attention.attention.value.lora_A\n",
      "vit.encoder.layer.10.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.10.attention.attention.value.lora_B\n",
      "vit.encoder.layer.10.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.10.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.10.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.10.attention.attention.dropout\n",
      "vit.encoder.layer.10.attention.output\n",
      "vit.encoder.layer.10.attention.output.dense\n",
      "vit.encoder.layer.10.attention.output.dropout\n",
      "vit.encoder.layer.10.intermediate\n",
      "vit.encoder.layer.10.intermediate.dense\n",
      "vit.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.10.output\n",
      "vit.encoder.layer.10.output.dense\n",
      "vit.encoder.layer.10.output.dropout\n",
      "vit.encoder.layer.10.layernorm_before\n",
      "vit.encoder.layer.10.layernorm_after\n",
      "vit.encoder.layer.11\n",
      "vit.encoder.layer.11.attention\n",
      "vit.encoder.layer.11.attention.attention\n",
      "vit.encoder.layer.11.attention.attention.query\n",
      "vit.encoder.layer.11.attention.attention.query.base_layer\n",
      "vit.encoder.layer.11.attention.attention.query.lora_dropout\n",
      "vit.encoder.layer.11.attention.attention.query.lora_dropout.default\n",
      "vit.encoder.layer.11.attention.attention.query.lora_A\n",
      "vit.encoder.layer.11.attention.attention.query.lora_A.default\n",
      "vit.encoder.layer.11.attention.attention.query.lora_B\n",
      "vit.encoder.layer.11.attention.attention.query.lora_B.default\n",
      "vit.encoder.layer.11.attention.attention.query.lora_embedding_A\n",
      "vit.encoder.layer.11.attention.attention.query.lora_embedding_B\n",
      "vit.encoder.layer.11.attention.attention.key\n",
      "vit.encoder.layer.11.attention.attention.value\n",
      "vit.encoder.layer.11.attention.attention.value.base_layer\n",
      "vit.encoder.layer.11.attention.attention.value.lora_dropout\n",
      "vit.encoder.layer.11.attention.attention.value.lora_dropout.default\n",
      "vit.encoder.layer.11.attention.attention.value.lora_A\n",
      "vit.encoder.layer.11.attention.attention.value.lora_A.default\n",
      "vit.encoder.layer.11.attention.attention.value.lora_B\n",
      "vit.encoder.layer.11.attention.attention.value.lora_B.default\n",
      "vit.encoder.layer.11.attention.attention.value.lora_embedding_A\n",
      "vit.encoder.layer.11.attention.attention.value.lora_embedding_B\n",
      "vit.encoder.layer.11.attention.attention.dropout\n",
      "vit.encoder.layer.11.attention.output\n",
      "vit.encoder.layer.11.attention.output.dense\n",
      "vit.encoder.layer.11.attention.output.dropout\n",
      "vit.encoder.layer.11.intermediate\n",
      "vit.encoder.layer.11.intermediate.dense\n",
      "vit.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.11.output\n",
      "vit.encoder.layer.11.output.dense\n",
      "vit.encoder.layer.11.output.dropout\n",
      "vit.encoder.layer.11.layernorm_before\n",
      "vit.encoder.layer.11.layernorm_after\n",
      "vit.layernorm\n",
      "classifier\n",
      "classifier.original_module\n",
      "classifier.modules_to_save\n",
      "classifier.modules_to_save.default\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T08:08:51.520159Z",
     "start_time": "2024-04-26T08:08:51.493180Z"
    }
   },
   "id": "ca98cbf0f953085c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c015d6eeb362c14e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "# import deeplabv3_resnet50\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the pretrained Vision Transformer\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "config = ViTConfig.from_pretrained(model_checkpoint)\n",
    "vit_model = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)  # No pooling to maintain spatial dimensions\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Use the values from the configuration file\n",
    "dataset_path = config['data_path']\n",
    "num_epochs = config['num_epochs']\n",
    "save_dir = config['save_dir']\n",
    "continue_training = config['continue_training']\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(vit_model, lora_config)\n",
    "\n",
    "\n",
    "class ViTBackbone(torch.nn.Module):\n",
    "    def __init__(self, vit_model):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        # Assume the output is a tuple (last_hidden_state, ...)\n",
    "        return outputs.last_hidden_state  # Returning the feature map\n",
    "\n",
    "\n",
    "# Replace the backbone in DeepLabV3\n",
    "model = deeplabv3_resnet50(weights=None, num_classes=20, aux_loss=True)\n",
    "model.backbone = ViTBackbone(lora_model)\n",
    "# Assuming ViT outputs 768-dimensional features\n",
    "model.classifier = DeepLabHead(768, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-26T08:19:31.647794Z"
    }
   },
   "id": "4b4e3bd92176d0c2"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589824 || all params: 86388480 || trainable%: 0.68\n",
      "trainable params: 9582632 || all params: 95381288 || trainable%: 10.05\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(lora_model)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T08:21:05.473021Z",
     "start_time": "2024-04-26T08:21:05.465035Z"
    }
   },
   "id": "fd3f03d37c23d217"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c36418c5a53ae338"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
